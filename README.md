# Efficient Join Operation in pyspark

This module provides utility functions to mitigate data skew problems commonly encountered when processing data with Apache Spark. It includes functionality to improve performance when joining large and small datasets.

## Features

- **Salted Join**: Mitigates data skew when joining a large dataframe with a small dataframe
- **Cross Join**: Performs efficient cross join using broadcast variables

## salted_join() Performance Improvement

| Join Type | Dataset Sizes | Built-in join() | salted_join() | Improvement |
|-----------|---------------|--------|-------|-------------|
| **Inner** | Df_big: ~400M<br>Df_small: 35K | 4m 20s | 1m 53s | **56.5%** |
| **Inner** | Df_big: ~400M<br>Df_small: 10K | 3m 2s | 1m 31s | **50.0%** |
| **Left**  | Df_big: ~400M<br>Df_small: 35K | 19.14s | 15.31s | **15.6%** |
| **Left**  | Df_big: ~400M<br>Df_small: 10K | 7.98s | 6.39s | **20.0%** |

## Summary

- **Inner Joins**: Improvement of approximately 40-50%
- **Left Joins**: Improvement of approximately 15-20%

## Main Functions

### `salted_join(df_big, big_key_column, df_small, small_key_column, join_type)`

Reduces data skew by adding "salt" to keys in the large dataframe and replicating the small dataframe for joining.

**Parameters:**
- `df_big`: Large Spark DataFrame
- `big_key_column`: Join key column name in the large dataframe
- `df_small`: Small Spark DataFrame
- `small_key_column`: Join key column name in the small dataframe
- `join_type`: Join type (must be one of: "inner", "left", "leftouter", "left_outer", "leftanti", "left_anti")

**Returns:**
- Joined Spark DataFrame

**Benefits:**
- **Resolves Data Skew**: Prevents processing bottlenecks caused by uneven data distribution across partitions
- **Improves Distributed Computing Efficiency**: By evenly distributing data across worker nodes, enables better parallelization
- **Prevents Executor Failures**: Reduces the risk of out-of-memory errors on specific executors handling skewed keys
- **Accelerates Processing Time**: Balanced workload leads to faster overall job completion

**Limitations:**
- Available join types are limited due to potential memory errors

### `crossjoin(df_big, df_small)`

Performs a cross join between a large dataframe and a small dataframe by broadcasting the small dataframe for efficiency.

**Parameters:**
- `df_big`: Large Spark DataFrame
- `df_small`: Small Spark DataFrame (will be broadcast)

**Returns:**
- Cross-joined Spark DataFrame

## How Salted Join Works

This utility addresses data skew problems using the following techniques:

1. **Key Salting**: Adding a randomized suffix (0-2) to keys in the large dataframe, effectively creating multiple virtual partitions for each key
2. **Small DataFrame Explosion**: Replicating each row in the small dataframe three times with corresponding suffixes to match with the salted keys
3. **Balanced Distribution**: Instead of having one partition process all records for a high-frequency key, the work is distributed across multiple partitions

The transformation process:
- Original join condition: `df_big.key = df_small.key`
- Transformed join condition: `df_big.key_salted = concat(df_small.key, "_", exploded_value)`

This approach ensures workload is evenly distributed across executor nodes in the Spark cluster, significantly improving parallel processing efficiency.

## Usage Examples

```python
from spark_utils import salted_join, crossjoin

# Example of salted join
result_df = salted_join(
    large_customer_df, 
    "customer_id", 
    small_product_df, 
    "customer_id", 
    "left"
)

# Example of cross join
result_cross_df = crossjoin(transaction_df, calendar_df)
```

## Performance Impact

The salted join approach may have the following impacts:

- **Positive**: Significant performance improvement for datasets with skewed key distributions
- **Positive**: Better resource utilization across the cluster
- **Consideration**: Slight overhead in preprocessing (adding salt and exploding the smaller dataset)
- **Consideration**: The number of salts (currently set to 3) may need tuning based on skew severity

## References

The development of this utility was informed by the following resources:

- [Why Your Spark Apps Are Slow Or Failing, Part II: Data Skew and Garbage Collection](https://dzone.com/articles/why-your-spark-apps-are-slow-or-failing-part-ii-da)
- [Spark's Salting â€” A Step Towards Mitigating Skew Problem](https://medium.com/curious-data-catalog/sparks-salting-a-step-towards-mitigating-skew-problem-5b2e66791620)

## Note
This readme was generated by Claude 3.7 Sonnet based on my codes and references
